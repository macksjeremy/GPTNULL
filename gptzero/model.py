import numpy as np
import torch
import re
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from collections import OrderedDict
from textattack import AttackArgs, Attacker
from textattack.models.wrappers import ModelWrapper, HuggingFaceModelWrapper, PyTorchModelWrapper
from textattack.datasets import Dataset
from textattack.attack_recipes import TextFoolerJin2019, PWWSRen2019
import nltk


class GPT2PPL:
    def __init__(self, device="cuda", model_id="gpt2"):
        self.device = device
        self.model_id = model_id
        self.model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_id)
        self.max_length = self.model.config.n_positions
        self.stride = 512

    def getResults(self, threshold):
        level = 140
        if threshold > level:
            threshold = level
        p1 = threshold/level
        p2 = (level - threshold)/level
        if threshold < 60:
            label = p1, p2
            return "The Text is generated by AI.", label
        elif threshold < 80:
            label = p1, p2
            return "The Text is most probably contain parts which are generated by AI. (require more text for better Judgement)", label
        else:
            label = p1, p2
            return "The Text is written by Human.", label

    def __call__(self, sentence):
        """
        Takes in a sentence split by full stop
        and print the perplexity of the total sentence
        split the lines based on full stop and find the perplexity of each sentence and print
        average perplexity
        Burstiness is the max perplexity of each sentence
        """
        results = OrderedDict()

        total_valid_char = re.findall("[a-zA-Z0-9]+", sentence)
        total_valid_char = sum([len(x) for x in total_valid_char])  # finds len of all the valid characters a sentence

        lines = re.split(r'(?<=[.?!][ \[\(])|(?<=\n)\s*', sentence)
        lines = list(filter(lambda x: (x is not None) and (len(x) > 0), lines))

        ppl = self.getPPL(sentence)
        results["Perplexity"] = ppl

        offset = ""
        Perplexity_per_line = []
        for i, line in enumerate(lines):
            if re.search("[a-zA-Z0-9]+", line) == None:
                continue
            if len(offset) > 0:
                line = offset + line
                offset = ""
            # remove the new line pr space in the first sentence if exists
            if line[0] == "\n" or line[0] == " ":
                line = line[1:]
            if line[-1] == "\n" or line[-1] == " ":
                line = line[:-1]
            elif line[-1] == "[" or line[-1] == "(":
                offset = line[-1]
                line = line[:-1]
            ppl = self.getPPL(line)
            Perplexity_per_line.append(ppl)
        if len(Perplexity_per_line) == 0:
            results["Perplexity per line"] = 0
        else:
            results["Perplexity per line"] = sum(Perplexity_per_line) / len(Perplexity_per_line)
        out, label = self.getResults(results["Perplexity per line"])
        results["label"] = label
        return label

    def getPPL(self, sentence):
        encodings = self.tokenizer(sentence, return_tensors="pt")
        seq_len = encodings.input_ids.size(1)

        nlls = []
        likelihoods = []
        prev_end_loc = 0
        for begin_loc in range(0, seq_len, self.stride):
            end_loc = min(begin_loc + self.max_length, seq_len)
            trg_len = end_loc - prev_end_loc
            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(self.device)
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            with torch.no_grad():
                outputs = self.model(input_ids, labels=target_ids)
                neg_log_likelihood = outputs.loss * trg_len
                likelihoods.append(neg_log_likelihood)

            nlls.append(neg_log_likelihood)

            prev_end_loc = end_loc
            if end_loc == seq_len:
                break
        perpl = torch.exp(torch.stack(nlls).sum() / end_loc)
        if torch.isnan(perpl):
            return 1
        ppl = int(perpl)
        return ppl


class MyModelWrapper(ModelWrapper):
    def __init__(self, model_param, InputTokenizer):
        self.model = model_param
        self.tokenizer = InputTokenizer
    def __call__(self, text_input_list):
        output = self.model(text_input_list)
        return output

# initialize the model
model_input = GPT2PPL()
#print("Model Initialized")
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
model = MyModelWrapper(model_input,tokenizer)
print("Model Wrapper initialized")
